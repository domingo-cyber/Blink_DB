BlinkDB Part A: In-Memory Key-Value Storage Engine Design Document
====================================================================

1. Overview
-----------
BlinkDB Part A implements a lightweight in-memory key-value storage engine inspired by Redis.
It supports basic CRUD operations through a simple REPL interface:
    • SET <key> "<value>"
    • GET <key>
    • DEL <key>
The primary goal is to achieve high performance for read, insert, and delete operations while 
ensuring that no query type becomes a bottleneck. Additionally, the design addresses the 
challenge of memory capacity by including provisions for data eviction and restoration.

2. Architecture
---------------
2.1 Storage Engine
    - Implemented as a C++ class that encapsulates an unordered_map.
    - Provides O(1) average-case performance for SET, GET, and DEL operations.
    - Uses an LRU (Least Recently Used) mechanism to manage memory efficiently.
    - Designed to be extended with future multi-threading capabilities.

2.2 REPL Interface
    - A command-line interface that reads user commands, passes them to the storage engine, 
      and displays the result.
    - Designed for easy integration with other components (e.g., a network layer in Part B).

3. Memory Capacity Management & Data Eviction
-----------------------------------------------
Since the database is in memory, it may eventually reach its capacity. The design addresses this 
by considering the following strategies:

3.1 Data Eviction Policy:
    - **Least Recently Used (LRU):**
        • The design includes an LRU cache mechanism.
        • Each key is stored in an ordered list (linked list).
        • When memory reaches the threshold, the least recently accessed keys are evicted.
    - **Trade-Offs:**
        • LRU provides high hit-rates for frequently accessed data but introduces additional overhead.
        • TTL-based flushing (not implemented yet) could provide automatic cleanup of expired keys.

3.2 Retrieval Strategy:
    - **Restoration of Evicted Data:**
        • Future implementations could include a write-ahead log or secondary storage for persistence.
    - **Cache Warm-Up:**
        • Frequently accessed keys could be preloaded at startup.

4. Workload Optimization
--------------------------
The design must be optimized for a particular workload. BlinkDB Part A is optimized for a 
"balanced" workload where read, write, and delete operations are equally important.

    • Balanced workloads are common in agile development systems and content management systems.
    • Optimizing solely for read-heavy might neglect write performance.
    • A balanced design ensures that SET, GET, and DEL operations are all efficient.

5. Data Structures and Specific Optimizations
-----------------------------------------------
5.1 Data Structures:
    - **Primary Data Structure:**  
      • Uses C++ std::unordered_map for key-value storage.  
      • Provides O(1) lookup, insertion, and deletion.
    - **LRU Mechanism:**  
      • A **doubly linked list** (std::list) tracks the order of key usage.  
      • An **unordered_map** stores iterators to the list for quick access.

5.2 Specific Optimizations:
    - **Minimized Overhead:** The REPL is simple, ensuring low processing overhead.
    - **Efficient Memory Management:** LRU handles automatic eviction when capacity is reached.
    - **Modular Design:** The storage engine can later support **multi-threading** or **persistence** 
      without major redesign.

6. Performance Evaluation and Benchmarking
--------------------------------------------
- The evaluation will use benchmark files containing a mixed workload of SET, GET, and DEL commands.
- **Performance Goals:**
    • SET (inserts) should be fast to support write operations.
    • GET (reads) must return data quickly.
    • DEL (deletes) should be efficient.
- Preliminary tests show **sub-millisecond latencies** for individual operations.
- **Future Optimization:** If eviction impacts performance, TTL values and cache sizes will be tuned.

7. Trade-Offs and Future Considerations
-----------------------------------------
- **Eviction Overhead vs. Data Freshness:**  
    • LRU introduces overhead but ensures frequently accessed data is retained.  
    • A TTL-based approach could be simpler but might remove useful data.
- **In-Memory vs. Persistent Storage:**  
    • The current implementation is fully in-memory.  
    • Future work may integrate a persistent store (e.g., database backup, disk storage).
- **Scalability Considerations:**  
    • Future versions may include **multi-threading** and **distributed storage** for large-scale 
      deployments.

8. Conclusion
-------------
BlinkDB Part A provides a high-performance in-memory key-value storage engine with a clean, 
modular design. It is optimized for **balanced workloads**, ensuring fast SET, GET, and DEL 
operations.

### Future Enhancements:
- Implement **persistent storage** for data recovery after crashes.
- Add **multi-threading** to handle concurrent requests.
- Optimize **eviction strategies** to balance memory usage and performance.

